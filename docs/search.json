[
  {
    "objectID": "posts/safety-estimand/index.html",
    "href": "posts/safety-estimand/index.html",
    "title": "Safety Estimand",
    "section": "",
    "text": "The concept of a safety estimand is gaining attention as researchers increasingly consider intercurrent events in the analysis of clinical trial safety data. In oncology trials, key intercurrent events include early treatment discontinuations due to inefficacy and intolerable toxicities, which often vary significantly between treatment groups. Employing various strategies to address these events effectively answers distinct clinical questions.\n\nHypothetical strategy: what is the increased risk of experiencing an adverse event (AE) for patients on experimental treatment versus comparator treatment without discontinuing?\nPrincipal stratum strategy: what is the increased risk of experiencing an AE for patients on experimental treatment vs. comparator treatment within the patient subgroup that would not discontinue?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Safety Analysis Blog",
    "section": "",
    "text": "Basic Introduction for Causal Inference\n\n\n\n\n\n\n\ncausal inference\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2025\n\n\nYujing Gao\n\n\n\n\n\n\n  \n\n\n\n\nIntercurrent Events\n\n\n\n\n\n\n\nintercurrent event\n\n\nstatistics\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2025\n\n\nYujing Gao\n\n\n\n\n\n\n  \n\n\n\n\nSafety Estimand\n\n\n\n\n\n\n\nsafety\n\n\nstatistics\n\n\nestimand\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2025\n\n\nWei Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Intercurrent_event/index.html",
    "href": "posts/Intercurrent_event/index.html",
    "title": "Intercurrent Events",
    "section": "",
    "text": "The general introduction of estimand framework includes four parts:\n\nTrial Objective\nEstimand\nMain Estimator\nMain Estimate\n\nAn estimand is a precise description of the treatment effect reflecting the clinical question posed by a given clinical trial objective, via specifying five attributes: Population, Treatment, Endpoint, Population-level Summary, Intercurrent Events.\nThe intercurrent event is an event that occurs after treatment initiation and affects either the interpretation or existence of the measurements associated with the clinical outcome.\nThere are five strategies (page 101-118 of ICH E9) to deal with intercurrent events: treatment policy, hypothetical, composite variable, while on treatment, principal stratum."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#the-general-introduction-of-estimand-framework",
    "href": "posts/Intercurrent_event/index.html#the-general-introduction-of-estimand-framework",
    "title": "Intercurrent Events",
    "section": "",
    "text": "The general introduction of estimand framework includes four parts:\n\nTrial Objective\nEstimand\nMain Estimator\nMain Estimate\n\nAn estimand is a precise description of the treatment effect reflecting the clinical question posed by a given clinical trial objective, via specifying five attributes: Population, Treatment, Endpoint, Population-level Summary, Intercurrent Events.\nThe intercurrent event is an event that occurs after treatment initiation and affects either the interpretation or existence of the measurements associated with the clinical outcome.\nThere are five strategies (page 101-118 of ICH E9) to deal with intercurrent events: treatment policy, hypothetical, composite variable, while on treatment, principal stratum."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#the-five-strategies",
    "href": "posts/Intercurrent_event/index.html#the-five-strategies",
    "title": "Intercurrent Events",
    "section": "2. The Five Strategies",
    "text": "2. The Five Strategies\n\n2.1 Treatment Policy\nThe treatment policy strategy includes all post-randomization data, regardless of the intercurrent events. In this approach, the primary analysis treats the occurrence of intercurrent events as part of the real-world scenario. Thus, data is collected and analyzed as if all patients followed the initial treatment assignment, irrespective of deviations. (Intention to treat - ITT)\nExample: A clinical trial where patients switch to a different medication due to adverse effects, but all data (pre- and post-switch) are included in the primary analysis.\n\n\n\n\n\n\n\n\n\n\n\n2.2 Hypothetical\nThe hypothetical strategy considers what the outcome would have been if the intercurrent event had not occurred. This strategy relies on assumptions to model the data based on a scenario where patients had continued as initially intended, disregarding the actual intercurrent events.\nExample: Estimating the effect of a drug assuming that all patients remained on the assigned treatment without any dropouts or additional medications.\nThe hypothetical scenario should consider reasonable situations, e.g. a scenario where a toxic medicine is considered to be non-toxic is not usually relevant for decision making.\n\n\n\n\n\n\n\n\n\n\n\n2.3 Composite Variable\nIn the composite variable strategy, the intercurrent event is integrated into the outcome itself, redefining it as part of a combined endpoint. This method allows for a new outcome that includes both the clinical endpoint and the intercurrent event, allowing analyses to capture their joint effect.\nExample: For a heart disease trial, combining hospitalization and mortality as a composite endpoint, rather than separating them, captures the total adverse outcomes.\n\n\n\n\n\n\n\n\n\n\n\n2.4 While-on-Treatment\nThe while-on-treatment strategy (also known as the on-treatment strategy) restricts analysis to data collected up until the occurrence of the intercurrent event. Once the intercurrent event occurs, further data is excluded from the analysis. This approach only considers the efficacy of the treatment while patients are actively taking it.\nExample: Including data from patients only while they adhere to the medication and excluding data collected after discontinuation or switch to another treatment.\n\n\n\n\n\n\n\n\n\n\n\n2.5 Principal Stratum\nThe principal stratum strategy analyzes only a specific subset of patients defined by their response to the intercurrent event, such as those who would not experience the event regardless of the treatment they received. This strategy requires assumptions about which patients would fall into this subset and generally involves a more complex statistical model.\nExample: In a study where some patients are likely to need additional therapy, the analysis might focus only on those who would not require additional therapy regardless of the treatment arm they are in."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#a-specific-example-need-to-change-for-safety-example",
    "href": "posts/Intercurrent_event/index.html#a-specific-example-need-to-change-for-safety-example",
    "title": "Intercurrent Events",
    "section": "3. A Specific Example (need to change for safety example)",
    "text": "3. A Specific Example (need to change for safety example)\nPrimary Estimand: to assess the immunogenicity of new vaccine (V1) as compared to old vaccine (V0) in healthy population who complete the vaccine regimen and do not experience events that could affect the immune response to vaccination."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#principal-stratification-method",
    "href": "posts/Intercurrent_event/index.html#principal-stratification-method",
    "title": "Intercurrent Events",
    "section": "4. Principal Stratification Method",
    "text": "4. Principal Stratification Method\n\n4.1 Principal Strata\nUnder the potential outcome framework (Rubin, 1974), each subject has potential outcomes \\(S(z)\\) and \\(Y(z)\\), where \\(z=0,1\\). Frangakis and Rubin (2002) defined the principal stratification variable, \\(U=S(1)S(0)\\), where \\(S(0),S(1)\\in\\{0,1\\}\\). Thus, there are four principal strata:\n\n\\(U=00\\): Never-adherence\n\\(U=10\\): Treatment-benefit adherence\n\\(U=01\\): Control-benefit adherence\n\\(U=11\\): Always-adherence\n\nTake \\(U=S(1)S(0) = 00\\) Never-adherence as an example:\n\n\\(S(1) = 0\\): when subject is assigned to the treatment group, she/he will not adhere to the protocol.\n\\(S(0) = 0\\): when subject is assigned to the control group, she/he will not adhere to the protocol.\n\nBelow we show the relationship between principal strata and the observed strata.\n\n\n\n\n\n\n\n\n\n\n\n4.2 Principal Causal Effects (PCEs)\nThe causal estimand here is defined as principal causal effects (PCEs): \\[\\tau_{U=S(1)S(0)} = E\\{Y(1) - Y(0) | U = S(1)S(0)\\},\\quad S(1)S(0) = 00, 10, 11, 01.\\]\nIn Section 2’s example, our interested parameter is PCE in Always-adherence strata: \\[\\tau_{11} = \\tau_{U=11} = E\\{Y(1) - Y(0)| U= 11\\}.\\]\n\n\n4.3 Per-protocol Method\nHere we also list the target estimand in Per-protocol method using our notation: \\[\\tau_{pp} = E(Y|Z=1,S=1) - E(Y|Z=0,S=1)\\]\nIn Section 2’s example, the interested parameter in the Per-protocol method corresponding to PCE in Always-adherence strata is: \\[\\tau_{11} = \\tau_{U=11} = E\\{Y(1) - Y(0)| U= 11\\}.\\]\nHowever, \\(\\tau_{11}\\) is defined on the potential principal strata \\(U=S(1)S(0) = 11\\), while \\(\\tau_{pp}\\) is defined on the observed strata \\((Z,S)\\).\nHere we can use a simple table to show the relationship between principal strata abd observed strata:\n\n\n\n\n\n\n\n\n\nThus, if we use the Per-protocol method to estimate \\(\\tau_{11}\\), \\(\\widehat{\\tau}_{pp}\\) may not be an consistent estimator for \\(\\tau_{11}\\) since it will be confounded by two other principal strata \\(U = 01\\) and \\(U=10\\).\n\n\n4.4 Current Principal Stratification Estimation Methods"
  },
  {
    "objectID": "posts/introduction_causal/index.html#potential-outcome-model",
    "href": "posts/introduction_causal/index.html#potential-outcome-model",
    "title": "Basic Introduction for Causal Inference",
    "section": "2. Potential Outcome Model",
    "text": "2. Potential Outcome Model\n\n2.1 Definition of Potential Outcomes and Causal Effects\nThe correlation between two variables \\(X\\) and \\(Y\\) can be represented using the join distribution of \\(X\\) and \\(Y\\). For example, the Pearson correlation coefficient is expressed as \\(\\rho(X,Y) = \\sigma_{xy}/\\sigma_{x}\\sigma_{y}\\), where $ _{xy}$ is the covariance between \\(X\\) and \\(Y\\), and \\(\\sigma_{x}, \\sigma_{y}\\) are the standard deviations of \\(X\\) and \\(Y\\), respectively. But how can we formally express the causal effect between X and Y? Using the joint distribution of the observed variables X and Y does not allow us to clearly define causal effects.\nStatisticians have used the potential outcome model to formalize the definition of causal effects. Using this framework, Neyman (1923) provided a mathematical definition of causal effects for experimental studies, and Rubin (1974) extended this definition to observational studies. The potential outcome model typically requires the Stable Unit Treatment Value Assumption (SUTVA): each individual’s potential outcomes are unaffected by the treatment assignments of others, and each individual has a well-defined outcome under each possible treatment. See Rubin (1980) for a detailed discussion. Consider a binary treatment or exposure variable T, where T = 1 denotes the treatment group and T = 0 denotes the control group. Let \\(Y(t)\\) denote the outcome under the treatment assignment \\(T=t\\), which is called the potential outcome. For each individual, the observed outcome variable Y can be written as \\(Y = TY(1) + (1-T) Y(0)\\). The SUTVA assumption means that the outcome is determined only by one’s own treatment and not by others’. However, this assumption may not hold in real-world settings. For instance, someone winning the lottery may impact their co-workers’ motivation; a friend getting a flu shot may indirectly reduce your own chance of getting infected. This assumption is one of the key limitations of the potential outcome model. Recently, some researchers have attempted to address this issue using social network methods (Athey et al., 2018; Eckles et al., 2017; Hudgens and Halloran, 2008; Liu and Hudgens, 2014; Sobel, 2006; Tchetgen and VanderWeele, 2012).\nCausal effects are typically defined as comparisons between an individual’s potential outcomes. The individual causal effect (ICE) for individual i is defined as:\n\\[ICE_i = Y_i(1) - Y_i(0).\\]\nAlthough the potential outcome model clearly defines individual causal effects, as Heraclitus once said: “You cannot step into the same river twice.” For each individual, we can only observe one of \\(Y_i(1)\\) or \\(Y_i(0)\\), not both. Therefore, individual causal effects cannot be directly observed and must be inferred from observed data. Despite this limitation, statisticians have proposed various methods to infer individual or subgroup causal effects, usually under strong modeling assumptions. Recently, individualized treatment effects and precision medicine have focused on identifying these effects (Chakraborty and Moodie, 2013; Kleinberg and Hripcsak, 2011; Murphy, 2003; Su et al., 2012). Since we cannot observe both potential outcomes for any one person, causal inference is inherently a problem of missing data. Statisticians are often more concerned with population-level features, such as the average causal effect, which can be defined using the potential outcome framework.\nStatistics is concerned with the characteristics of a population. Using potential outcomes, we can also define the average causal effect for the population.\nDefinition 2.1 The Average Causal Effect (ACE) for the population is defined as the expectation of the individual causal effect:\n\\[ACE = E(ICE) = E[Y(1) - Y(0)] = E[Y(1)] - E[Y(0)].\\]\nThe average causal effect is defined as the difference between the average outcome \\(E[Y(1)]\\) if all individuals were assigned to treatment \\(T=1\\) and the average outcome \\(E[Y(0)]\\) if all individuals were assigned to control \\(T=0\\). In practice, it is impossible to have all individuals receive treatment \\(T=1\\) and then later receive control \\(T=0\\); even if this were attempted, for the same individual i, the potential outcome \\(Y_i(t)\\) when first receiving treatment \\(T=t\\) might differ from the potential outcome \\(Y_i(t)^\\prime\\) when receiving treatment \\(T=t\\) later.\nFurthermore, one might be interested in the average causal effect within a specific subpopulation. For example, the efficacy of a certain drug might differ for different groups, such as males or females.\nDefinition 2.2 Let X be a covariate. The average causal effect for the subpopulation with \\(X=x\\) is defined as:\n\\[E[Y(1) - Y(0)|X = x].\\]\nAdditionally, people are often concerned with the causal effect specifically for the treated group. For instance, epidemiologists might not be interested in the causal effect of smoking on the entire population, but only on the subpopulation who actually smoke.\nDefinition 2.3 The Average Causal effect for the Treated (ATT) is defined as:\n\\[E[Y(1) - Y(0)|T=1].\\] We say the average causal effect:\n\\[ACE = E[Y(1) - Y(0)]\\] is identifiable if the ACE can be uniquely determined from the distribution \\(P(T,Y,X)\\) of the observed variables. If the ACE is not identifiable, it means there exist at least two unequal values \\(ACE \\neq ACE^\\prime\\) that are both consistent with the observed data. Identifiability is often the most challenging issue in causal inference. To achieve identifiability of causal effects, additional assumptions are typically required. Randomized experiments are the most effective method for identifying causal effects."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#an-simple-example",
    "href": "posts/Intercurrent_event/index.html#an-simple-example",
    "title": "Intercurrent Events",
    "section": "3. An Simple Example",
    "text": "3. An Simple Example\nPrimary Estimand: to assess the outcome of new medicine (treatment group) as compared to old medicine (control group) in defined population who complete the treatment regimen."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#reference",
    "href": "posts/Intercurrent_event/index.html#reference",
    "title": "Intercurrent Events",
    "section": "Reference",
    "text": "Reference\n\nD. B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. J Educ Psychol, 66(5): 688–701, 1974.\nC.E. Frangakis and D. B. Rubin. Principal stratification in causal inference. Biometrics, 58(1):21–29, 2002.\nP. Ding and J. Lu. Principal stratification analysis using principal scores. Journal of the Royal Statistical Society Series B: Statistical Methodology, 79(3):757–777, 2017.\nZ. Jiang, S. Yang, and P. Ding. Multiply robust estimation of causal effects under principal ignorability. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4):1423–1445, 2022."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#principal-stratification-framework",
    "href": "posts/Intercurrent_event/index.html#principal-stratification-framework",
    "title": "Intercurrent Events",
    "section": "4. Principal Stratification Framework",
    "text": "4. Principal Stratification Framework\n\n4.1. Principal Strata\nUnder the potential outcome framework (Rubin, 1974), each subject has potential outcomes \\(S(z)\\) and \\(Y(z)\\), where \\(z=0,1\\). Frangakis and Rubin (2002) defined the principal stratification variable, \\(U=S(1)S(0)\\), where \\(S(0),S(1)\\in\\{0,1\\}\\). Thus, there are four principal strata:\n\n\\(U=00\\): Never-adherence\n\\(U=10\\): Treatment-benefit adherence\n\\(U=01\\): Control-benefit adherence\n\\(U=11\\): Always-adherence\n\nTake \\(U=S(1)S(0) = 00\\) Never-adherence as an example:\n\n\\(S(1) = 0\\): when subject is assigned to the treatment group, she/he will not adhere to the protocol.\n\\(S(0) = 0\\): when subject is assigned to the control group, she/he will not adhere to the protocol.\n\nBelow we show the relationship between principal strata and the observed strata.\n\n\n\n\n\n\n\n\n\n\n\n4.2. Principal Causal Effects (PCEs)\nThe causal estimand here is defined as principal causal effects (PCEs): \\[\\tau_{U=S(1)S(0)} = E\\{Y(1) - Y(0) | U = S(1)S(0)\\},\\quad S(1)S(0) = 00, 10, 11, 01.\\]\nIn Section 2’s example, our interested parameter is PCE in Always-adherence strata: \\[\\tau_{11} = \\tau_{U=11} = E\\{Y(1) - Y(0)| U= 11\\}.\\]\n\n\n4.3. Per-protocol Method\nHere we also list the target estimand in Per-protocol method using our notation: \\[\\tau_{pp} = E(Y|Z=1,S=1) - E(Y|Z=0,S=1)\\]\nIn Section 2’s example, the interested parameter in the Per-protocol method corresponding to PCE in Always-adherence strata is: \\[\\tau_{11} = \\tau_{U=11} = E\\{Y(1) - Y(0)| U= 11\\}.\\]\nHowever, \\(\\tau_{11}\\) is defined on the potential principal strata \\(U=S(1)S(0) = 11\\), while \\(\\tau_{pp}\\) is defined on the observed strata \\((Z,S)\\).\nHere we can use a simple table to show the relationship between principal strata abd observed strata:\n\n\n\n\n\n\n\n\n\nThus, if we use the Per-protocol method to estimate \\(\\tau_{11}\\), \\(\\widehat{\\tau}_{pp}\\) may not be an consistent estimator for \\(\\tau_{11}\\) since it will be confounded by two other principal strata \\(U = 01\\) and \\(U=10\\)."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#current-principal-stratification-estimation-methods",
    "href": "posts/Intercurrent_event/index.html#current-principal-stratification-estimation-methods",
    "title": "Intercurrent Events",
    "section": "5 Current Principal Stratification Estimation Methods",
    "text": "5 Current Principal Stratification Estimation Methods\n\n5.1"
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#principal-stratification-estimation-methods",
    "href": "posts/Intercurrent_event/index.html#principal-stratification-estimation-methods",
    "title": "Intercurrent Events",
    "section": "5. Principal Stratification Estimation Methods",
    "text": "5. Principal Stratification Estimation Methods\n\n5.1. Most Recent Principal Stratification Methods:\nHere we first list the most recent principal stratification methods and provide one plot to show the relationship among these methods.\n\n\n\n\n\n\n\n\n\n\n\n5.2 Identification Assumptions\n\nAssumption 1: SUTVA (Stable Unit Treatment Value Assumption): There is no interference and no hidden variations of treatment.\n\nThis assumption hold in the completely randomized trial.\n\nAssumption 2: Treatment Ignorability: \\(Z\\perp \\{S(0), S(1), Y(0), Y(1)\\}| \\boldsymbol{X}\\).\n\nThis assumption hold in the completely randomized trial.\n\nAssumption 3: Monotonicity: \\(S(1)\\geq S(0)\\).\n\nThis rules out stratum U=01 (Control-benefit adherence).\nThis means: when participants are assigned to the treatment group, they are more likely to adhere to protocol comparing with when participants are assigned to the control group.\nUnder Assumption 3, the relationship table between the principal strata and the observed strata changed.\nNow \\(\\tau_{pp}\\) is still confounded by the principal strata \\(U=10\\).\n\n\n\n\n\n\n\n\n\n\n\n\nAssumption 4: Principal Ignorability: \\[E\\{Y(1) | U=11, \\boldsymbol{X}\\} = E\\{Y(1) | U=10, \\boldsymbol{X}\\} \\] \\[E\\{Y(0) | U=00, \\boldsymbol{X}\\} = E\\{Y(0) | U=10, \\boldsymbol{X}\\} \\]\n\nThis requires that the expectations of the potential outcome do not vary across some principal strata U conditional on the covariates.\nTake the first formula as an example. Under Assumption 1-3, the first formula is equivalent to \\[\\begin{align*}\nZ=1,S=1:\\quad  & E\\{Y(1)| U = 11, Z=1,S=1, \\boldsymbol{X}\\} \\\\\n= &  E\\{Y(1)| U = 10, Z=1, S=1, \\boldsymbol{X}\\}\\\\\n= &  E\\{Y | Z=1,S=1,\\boldsymbol{X}\\}.\n\\end{align*}\\]\nThis assumption help build an bridge to connect observed data with potential outcome. Thus, we can use observed data to estimate the principal causal effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3 Principal Score Methods (Ding et al.; 2017)\n\n\n\n\n\n\n\n\n\n\n\n5.4 Multiple Robust Methods (Jiang et al.; 2017)\nFirst, we list two estimators that rely on propensity score model for Z and principal score model for S.\n\n\n\n\n\n\n\n\n\nNext, we introduce two estimators that rely on outcome mean model for Y, where also relies on propensity score model for Z and relies on principal score model for S.\n\n\n\n\n\n\n\n\n\nFinally, we present the triply robust estimator, which relies on all three models: propensity score model for Z, principal score model for S and outcome mean mdoel for Y."
  },
  {
    "objectID": "posts/Intercurrent_event/index.html#simple-simulation",
    "href": "posts/Intercurrent_event/index.html#simple-simulation",
    "title": "Intercurrent Events",
    "section": "6. Simple Simulation",
    "text": "6. Simple Simulation\nIn this section, we use an simple simulation to show the codes for these principal stratification estimation methods and the Per-protocol method.\n\n6.1 Setting\nWe follow the setting in Section 2.1, where we have three baseline variables:\n\n\\(X_1\\): Sex, binary, Bernoulli distribution with probability 0.5 as 1 (female)\n\\(X_2\\): Some transformed variable, binary, Bernoulli distribution with probability 0.8 as 1\n\\(X_3\\): Age, continuous, normal distribution with mean 50 and variance 8\n\nIn addition, we use the logistic model to generate the intercurrent event:\n\\[P(S=1|Z, \\boldsymbol{X}) = logistic(-1 + 0.15Z +0.6X_1 + 0.8 X_2 + 0.05*X_3 )\\] and we use the linear regression model to generate the observed outcome:\n\\[ Y = 0.5 - 0.7Z + 0.2S - 0.1X_1 +0.2 X_2 + 0.05X_3 + 0.02X_3Z - 0.03 X_3 S + \\epsilon\\] where \\(\\epsilon\\sim N(0, \\sigma^2 = 0.6^2Z + 0.65^2(1-Z))\\).\nUnder this simulation setting, we provide the true values for both the PCEs and the PP.\n\n# need to add true value\n\n\nlibrary(dplyr)\n\n###################################\n######  Simulation function  ######\n\n# Based on vaccine data, we have 3 baseline covariates:\n#  Sex (binary), transformed variable (binary), age (continuous)\nf_sim &lt;- function(N = 200*2, # sample size for two groups\n                  seed = 123, # random seed\n                  pi_Z = 0.5, # treatment propensity score: ratio of treatment / control, constant\n                  ctrl_male_p = 0.5, # ratio of male (control)\n                  trt_male_p = 0.5, # ratio of male (treat)\n                  ctrl_trans_p = 0.8, # ratio of transformed variable (control)\n                  trt_trans_p = 0.8, # ratio of transformed variable (treat)\n                  ctrl_age_mu = 50, # mean of age (control)\n                  ctrl_age_sigma = 8, # sd of age (control)\n                  trt_age_mu = 50, # mean of age (treat)\n                  trt_age_sigma = 8, # sd of age (treat)\n                  S_coef = c(-1, # intercept,\n                             0.15, # coef of Z, previous 0.18991, 2 for part2\n                             0.6, # coef of Sex, previous 0.57866\n                             0.8, # coef of Trans, previous 0.77246\n                             0.05 # coef of Age, previous 0.03093\n                             # 0.3 # coef of unmeasured confounder\n                  ), # coef of S's model - c(intercept, Z, Sex, Trans, Age)\n                  Y_coef = c(0.5, # intercept,\n                             -0.7, # coef of Z\n                             0.2, # coef of S\n                             -0.1, # coef of Sex\n                             0.2, # coef of Trans\n                             0.05, # coef of Age\n                             0.02, # coef of Age * Z\n                             -0.03 # coef of Age * S \n                  ), \n                  sd_Y_trt = 0.6,\n                  sd_Y_ctrl = 0.65\n){\n  \n  logit &lt;- function(y){\n  exp(y)/ (exp(y) + 1)\n  }\n  \n  set.seed(seed)\n  Nt &lt;- N * pi_Z # sample size of treatment group\n  Nc &lt;- N * (1 - pi_Z)  # sample size of control group\n  \n  ## Generate baseline covariates\n  # Generate data for the treatment group\n  Trt_Z &lt;-  rep(1, Nt)\n  Trt_sex &lt;- rbinom(n = Nt, size = 1, prob = trt_male_p) # 1 means male, 0 means female\n  Trt_trans &lt;- rbinom(n = Nt, size = 1, prob = trt_trans_p) \n  Trt_age &lt;- rnorm(n = Nt, mean = trt_age_mu, sd = trt_age_sigma)\n  \n  # Generate data for the control group\n  Ctrl_Z &lt;-  rep(0, Nc)\n  Ctrl_sex &lt;- rbinom(n = Nc, size = 1, prob = ctrl_male_p) \n  Ctrl_trans &lt;- rbinom(n = Nc, size = 1, prob = ctrl_trans_p) \n  Ctrl_age &lt;- rnorm(n = Nc, mean = ctrl_age_mu, sd = ctrl_age_sigma)\n  \n  ## Generate intermediate variable A = 1, adherence\n  Trt_X &lt;- matrix(c(rep(1, Nt), \n                    Trt_Z,\n                    Trt_sex,\n                    Trt_trans,\n                    Trt_age),\n                    nrow = Nt, \n                    byrow = F)\n  Trt_link_S &lt;-  Trt_X %*% S_coef\n  Trt_prob_S &lt;- logit(Trt_link_S)\n  Trt_S &lt;- rbinom(length(Trt_prob_S), 1, Trt_prob_S)\n  \n  Ctrl_X &lt;- matrix(c(rep(1, Nc), \n                     Ctrl_Z,\n                     Ctrl_sex,\n                     Ctrl_trans,\n                     Ctrl_age),\n                   nrow = Nc, \n                   byrow = F)\n  Ctrl_link_S &lt;-  Ctrl_X %*% S_coef\n  Ctrl_prob_S &lt;- logit(Ctrl_link_S)\n  Ctrl_S &lt;- rbinom(length(Ctrl_prob_S), 1, Ctrl_prob_S)\n  \n  ## Generate outcome Y - full model\n  # generate the residual part\n  Trt_res &lt;- rnorm(Nt, mean = 0, sd = sd_Y_trt) \n  Ctrl_res &lt;- rnorm(Nc, mean = 0, sd = sd_Y_ctrl)\n  \n  Trt_X_Y &lt;- matrix(c(rep(1, Nt), \n                      Trt_Z,\n                      Trt_S,\n                      Trt_sex,\n                      Trt_trans,\n                      Trt_age,\n                      Trt_age * Trt_Z,\n                      Trt_age * Trt_S),\n                    nrow = Nt, \n                    byrow = F)\n  Trt_Y &lt;- Trt_X_Y %*% Y_coef + Trt_res\n  \n  Ctrl_X_Y &lt;- matrix(c(rep(1, Nc), \n                       Ctrl_Z,\n                       Ctrl_S,\n                       Ctrl_sex,\n                       Ctrl_trans,\n                       Ctrl_age,\n                       Ctrl_age * Ctrl_Z,\n                       Ctrl_age * Ctrl_S),\n                     nrow = Nc, \n                     byrow = F)\n  Ctrl_Y &lt;- Ctrl_X_Y %*% Y_coef + Ctrl_res\n\n  data &lt;- data.frame(\n    Z = c(Ctrl_Z, Trt_Z),\n    Sex = c(Ctrl_sex, Trt_sex),\n    Ges = c(Ctrl_trans, Trt_trans),\n    Age = c(Ctrl_age, Trt_age),\n    S = c(Ctrl_S, Trt_S),\n    Y = c(Ctrl_Y, Trt_Y)\n  )\n  \n  return(data)\n  \n}\n\ndata_simu &lt;- f_sim(N = 1000*2)\nhead(data_simu)\n\n  Z Sex Ges      Age S        Y\n1 0   0   1 48.79754 1 1.421453\n2 0   1   0 47.37794 0 3.416591\n3 0   1   1 38.41468 1 1.118009\n4 0   1   1 44.42172 1 1.621170\n5 0   0   1 70.78792 1 2.708271\n6 0   0   1 49.70068 1 1.498784\n\n\n\n\n6.2 Estimation\nHere we show R codes for principal stratification estimation. First, for the two estimation methods provided by Ding et al. (2017), below is the original R code provided within their paper:\n\n# Ding's original estimation code\n{\n  ##################################################################\n  ######################## EM Algorithm for ########################\n  #### Principal Stratification Analysis using Propensity Score ####\n  ######################## With Monotonicity #######################\n  ###################### Ding and Lu 2015 Oct ######################\n  ##################################################################\n  \n  \n  ##propensity score method, with covariate adjustment and sensitivity analysis for GPI\n  #the package used for multivariate logistic regression\n  library(nnet)\n  \n  #Preliminary function: principal score calculation \n  #Z: randomization\n  #D: treatment received\n  #X: pretreatment covaraites, 11111 in the first column\n  #beta.a, beta.n: initial values for the paramaters in the multiple logistic regression\n  #iter.max: total number of iterations\n  #error0: convergence error rate\n  #Trace: if TRUE then trace each EM iteration\n  #fitting multinomial logistic regression model with principal stratification variable as missing data\n  PS_pred = function(Z, D, X, \n                     beta.a = NULL, beta.n = NULL, \n                     iter.max = 200, error0 = 10^-6, Trace = FALSE) {  \n    V = dim(X)[2]\n    N = length(Z)\n    if(is.null(beta.a)) beta.a = rep(0, V)\n    if(is.null(beta.n)) beta.n = rep(0, V)  \n    \n    iter = 1         \n    repeat{\n      \n      ##initial values of iteration\n      beta.a_old = beta.a\n      beta.n_old = beta.n\n      \n      if(Trace == T) {\n        print(paste(\"The \", iter, \"-th EM iteration!\", sep=\"\"))\n      }\n      \n      #E step: posterior probabilities\n      #and the augmented data set with weights\n      #creat a null matrix for the augmented data set AugData\n      AugData = NULL\n      #each individual correspond to 1 or 2 individuals in the augmented data set\n      for(i in 1:N) {\n        if(Z[i]==1&D[i]==1) {\n          #posterior probabilities\n          prob.c = 1/(1 + exp(t(beta.a_old)%*%X[i, ]))\n          prob.a = 1 - prob.c\n          \n          AugData = rbind(AugData, c(1, X[i, ], prob.c))\n          AugData = rbind(AugData, c(2, X[i, ], prob.a))\n        }\n        \n        if(Z[i]==1&D[i]==0) {\n          AugData = rbind(AugData, c(3, X[i, ], 1))  \n        }\n        \n        if(Z[i]==0&D[i]==1) {\n          AugData = rbind(AugData, c(2, X[i, ], 1))  \n        }\n        \n        if(Z[i]==0&D[i]==0) {\n          #posterior probabilities\n          prob.c = 1/(1 + exp(t(beta.n_old)%*%X[i, ]))\n          prob.n = 1 - prob.c\n          \n          AugData = rbind(AugData, c(1, X[i, ], prob.c))\n          AugData = rbind(AugData, c(3, X[i, ], prob.n))  \n          \n        }#for if\n        \n      }#for \"for\"\n      #make AugData into a dataframe\n      #AugData = data.frame(AugData)\n      #colnames(AugData) = c(\"U\", \"X\", \"Weight\")\n      #Multinomial logistic regression using \"nnet\" package\n      \n      fit = multinom(AugData[, 1] ~ AugData[, (3:(V+1))], weights = AugData[, (V+2)], trace = FALSE)\n      betas  = coef(fit)\n      beta.a = betas[1, ]\n      beta.n = betas[2, ]\n      \n      iter = iter + 1\n      error = sum((beta.a - beta.a_old)^2)  + sum((beta.n - beta.n_old)^2)\n      if(iter&gt;iter.max||error&lt;error0)   break           \n      \n    }#for repeat\n    \n    #the predicted probabilities\n    #three columns corresponding to complier, always taker and never taker\n    PROB = matrix(0, N, 3)\n    for(i in 1:N) {\n      prob.c = 1\n      prob.a = exp(t(beta.a)%*%X[i, ])\n      prob.n = exp(t(beta.n)%*%X[i, ])\n      sum = prob.c + prob.a + prob.n\n      \n      PROB[i,] = c(prob.c, prob.a, prob.n)/sum\n    }\n    \n    results = list(PROB=PROB, beta.a=beta.a, beta.n=beta.n)\n    return(results)\n  }\n  \n  #Main function\n  #Z: randomization\n  #D: treatment received\n  #X: covariate matrix: the first column is NOT 11111\n  #U: (latent) principal stratification variable, 1 complier, 2 always taker, 3 never taker\n  #Y: outcome of interest\n  #trc: truncation by death indicator, default FALSE. If TRUE only SACE (i.e. AACE) is calculated.\n  #ep1, ep0: sensitivity parameters in Proposition 4, Section 6.1.\n  #beta.a, beta.n: initial values for the paramaters in the multiple logistic regression\n  PSPS_M_weighting = function(Z, D, X, Y, \n                              trc = FALSE, ep1 = 1, ep0 = 1,\n                              beta.a = NULL, beta.n = NULL) {\n    #augment the design X\n    N = length(Z)\n    X = cbind(rep(1, N), X)\n    \n    #estimate the propensity scores using Multinomial Logistic Regression\n    #PS_pred returns three columns: c, a, n\n    ps.score.fit = PS_pred(Z, D, X, beta.a = beta.a, beta.n = beta.n)\n    ps.score     = ps.score.fit$PROB\n    pr.n = sum(Z*(1 - D))/sum(Z)\n    pr.a = sum((1 - Z)*D)/sum(1-Z)\n    pr.c = 1 - pr.n - pr.a\n    \n    #indices\n    index11 = (1:N)[Z==1&D==1]\n    index10 = (1:N)[Z==1&D==0]\n    index01 = (1:N)[Z==0&D==1]\n    index00 = (1:N)[Z==0&D==0]\n    \n    #weights\n    if (trc == F) {\n      w1c = ep1*ps.score[index11, 1]/(ep1*ps.score[index11, 1] + ps.score[index11, 2])/pr.c*(pr.c + pr.a)\n      w0c = ep0*ps.score[index00, 1]/(ep0*ps.score[index00, 1] + ps.score[index00, 3])/pr.c*(pr.c + pr.n)\n      w0n = ps.score[index00, 3]/(ep0*ps.score[index00, 1] + ps.score[index00, 3])/pr.n*(pr.c + pr.n)\n    }\n    w1a = ps.score[index11, 2]/(ep1*ps.score[index11, 1] + ps.score[index11, 2])/pr.a*(pr.c + pr.a)\n    \n    #model assisted regression estimator \n    if (trc == F) {\n      r1c = lm(Y[index11] ~ 0 + X[index11, ], weights = w1c)$coef\n      r0c = lm(Y[index00] ~ 0 + X[index00, ], weights = w0c)$coef\n      r1n = lm(Y[index10] ~ 0 + X[index10, ])$coef\n      r0n = lm(Y[index00] ~ 0 + X[index00, ], weights = w0n)$coef\n    }\n    r1a = lm(Y[index11] ~ 0 + X[index11, ], weights = w1a)$coef\n    r0a = lm(Y[index01] ~ 0 + X[index01, ])$coef\n    \n    #weighted outcomes\n    if (trc == F) {\n      weighted.Y.c1 = Y[index11]*w1c\n      weighted.Y.c0 = Y[index00]*w0c\n      weighted.Y.n0 = Y[index00]*w0n\n    }\n    weighted.Y.a1 = Y[index11]*w1a\n    \n    #CACE, NACE and AACE\n    if (trc == F) {\n      CACE = mean(weighted.Y.c1) - mean(weighted.Y.c0)\n      NACE = mean(Y[index10]) - mean(weighted.Y.n0)\n    }\n    AACE = mean(weighted.Y.a1) - mean(Y[index01])\n    \n    #weighted outcomes for regression estimator\n    if (trc == F) {\n      weighted.Y1c = (Y[index11]-X[index11, ]%*%r1c)*w1c\n      weighted.Y0c = (Y[index00]-X[index00, ]%*%r0c)*w0c\n      weighted.Y1n = Y[index10]-X[index10, ]%*%r1n\n      weighted.Y0n = (Y[index00]-X[index00, ]%*%r0n)*w0n\n      weighted.rc = rbind(X[index11, ]*w1c, X[index00, ]*w0c) %*% (r1c - r0c)\n      weighted.rn = rbind(X[index10, ], X[index00, ]*w0n) %*% (r1n - r0n)\n    }\n    weighted.Y1a = (Y[index11]-X[index11, ]%*%r1a)*w1a\n    weighted.Y0a = Y[index01]-X[index01, ]%*%r0a\n    weighted.ra = rbind(X[index11, ]*w1a, X[index01, ]) %*% (r1a - r0a)\n    \n    #CACE, NACE and AACE, regression estimates\n    if (trc == F) {\n      CACE.reg = mean(weighted.Y1c) - mean(weighted.Y0c) + mean(weighted.rc)\n      NACE.reg = mean(weighted.Y1n) - mean(weighted.Y0n) + mean(weighted.rn)\n    }\n    AACE.reg = mean(weighted.Y1a) - mean(weighted.Y0a) + mean(weighted.ra)\n    \n    #results\n    if (trc == F) {\n      ACE = list(CACE = CACE, CACE.reg = CACE.reg, \n                 NACE = NACE, NACE.reg = NACE.reg, \n                 AACE = AACE, AACE.reg = AACE.reg,  \n                 beta.a = ps.score.fit$beta.a, beta.n = ps.score.fit$beta.n)\n    }\n    else {\n      ACE = list(AACE = AACE, AACE.reg = AACE.reg,  \n                 beta.a = ps.score.fit$beta.a, beta.n = ps.score.fit$beta.n)\n    }\n    return(ACE)\n    \n  }\n}\n\nGiven their estimation functions, we use the simulated data data_simu to show the application\n\nres_PS &lt;- PSPS_M_weighting(Z = data_simu$Z, \n                           D = data_simu$S, \n                           X = as.matrix(data_simu[, 2:4]), \n                           Y = data_simu$Y,\n                           trc = FALSE, \n                           ep1 = 1, \n                           ep0 = 1,\n                           beta.a = NULL,\n                           beta.n = NULL)\n  \nres_PS_AACE &lt;- c(res_PS$AACE, res_PS$AACE.reg)\nres_PS_CACE &lt;- c(res_PS$CACE, res_PS$CACE.reg)\nres_PS_NACE &lt;- c(res_PS$NACE, res_PS$NACE.reg)\n\nres_PS_AACE\n\n[1] 0.3719683 0.3565214\n\nres_PS_CACE\n\n[1] -0.9446866 -0.8833508\n\nres_PS_NACE\n\n[1] 0.1765227 0.2109585\n\n\nNext, we present the R code for the five estimation methods provided by Jiang et al. (2022), which is in their R pcakge pace. This package can be installed using the following R code and can be found on web page https://github.com/shuyang-stat/pace.\n\ndevtools::install_github(\"shuyang1987/pace\")\n\n\nlibrary(pace)\n    \nres_MR &lt;-  pace::pace(X = as.matrix(data_simu[, c(2:4)]),\n                        Z = data_simu$Z,\n                        S = data_simu$S,\n                        Y = data_simu$Y,\n                        family.Y=\"gaussian\",\n                        nboot = 500 # bootstrap times\n                      )\n\n# For AACE:\nres_MR_AACE &lt;- c(res_MR$tau11w, res_MR$tau11sw, res_MR$tau11reg, res_MR$tau11reg2, res_MR$tau11aw)\nres_MR_AACE\n\n[1] 0.3539948 0.3559021 0.3561363 0.3560903 0.3560693\n\n# For CACE:\nres_MR_CACE &lt;- c(res_MR$tau10w, res_MR$tau10sw, res_MR$tau10reg, res_MR$tau10reg2, res_MR$tau10aw)\nres_MR_CACE\n\n[1] -0.9195095 -0.9319140 -0.9541461 -0.9531794 -0.9512701\n\n# For NACE:\nres_MR_CACE &lt;- c(res_MR$tau00w, res_MR$tau00sw, res_MR$tau00reg, res_MR$tau00reg2, res_MR$tau00aw)\nres_MR_CACE\n\n[1] 0.2355574 0.1943853 0.1989377 0.1993048 0.2002920\n\n\nwhere 11 corresponds to AACE, 10 corresponds to CACE, 00 corresponds to NACE."
  }
]